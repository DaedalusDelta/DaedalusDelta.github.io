---
layout: post
title:  "From DextrAH-G to DextrAH-RGB"
date:   2025-07-21
video:  assets/videos/dextrah_blog.mp4
tags:   [deep_learning, robot_manipulation, sim2real, teacher-student, isaaclab]
---

# Story
DextrAH-G presents the following contributions
1. Efficient sim2real scaling for vectorized RL training
2. Using geometry fabrics for safe policies

Problems
1. each object is specified by a one-hot embedding for the teacher
2. different random seeds can result in vastly different learned behaviors, from which the authors need to handpick the best one, meaning that there is no replacement for real-world testing (gotta train multiple teacher policies and critics, each with different behavior)

# Geometry Fabrics
geometric fabrics generalize the behavior of classical mechanical systems and, thereby, can be used to model controllers with design flexibility, composability, and stability without the loss of modeling fidelity. Behavior expressed by a geometric fabric follows the form

$$
M_f(q_f, \dot{q}_f)\ddot{q}_f+f_f(q_f, \dot{q}_f)+f_\pi(a)=0
$$

where $M_f\in\mathbb{R}^{n\times n}$ is the positive-definite system metric (mass), which captures system prioritization, $f_f\in\mathbb{R}^n$ is a nominal path generating geometric force, and $f_\pi(a)\in\mathbb{R}^n$ is an additional driving force of some action $a\in\mathbb{R}^m$. $q_f, \dot{q}_f, \ddot{f}\in\mathbb{R}^n$ are the position, velocity, and acceleration of the fabric.

# Dataset
trained on 140 diverse objects in Visual Dexterity object dataset
object meshes are preprocessed by computing the mesh centroid using Trimesh and transforming the vertices such that the new mesh centroid is at the origin, so that the object positions given to the simulator would be exactly at the object centroids


# Methods
Asymmetric Actor Critic training

### teacher privileged training
The critic $V(s)$ is given privileged state info $s$ and the teacher policy $\pi_{privileged}(o_{privileged})$ is provided an observation $o_{privileged}=[o_{robot}, x_{goal}, o_{obj}]$. $o_{robot}$ includes the cspace position $q\in\mathbb{R}^{N_q}$, cspace velocity $\dot{q}\in\mathbb{R}^{N_q} (N_q=23)$, positions of three points on the palm $[x_{palm}, x_{palm-x}, x_{palm-y}]\in\mathbb{R}^{3\times 3}$, positions of the $N_{fingers}=4$ fingertips $x_{fingertips}\in\mathbb{R}^{N_{fingers}\times 3}$, and the fabric state $[q_f, \dot{q}_f, \ddot{q}_f]\in\mathbb{R}^{3\times N_q}$. $x_{goal}\in\mathbb{R}^3$ is the goal object position. $o_{obj}$ includes the noisy object position $\tilde{x}_{obj}\in\mathbb{R}^3$ and quaternion $\tilde{q}_{obj}\in\mathbb{R}^4$, and the object one-hot embedding $e\in\{0, 1\}^{N_{objects}}$, where $N_{objects}=140$ is the number of objects in the training dataset.
We define the critic's input state as $s=[o_{privileged}, s_{privileged}]$. $s_{privileged}$ contains privileged state information including robot joint forces $f_{dof}\in\mathbb{R}^{N_q}$, fingertip contact forces $f_{fingers}\in\mathbb{R}^{N_{fingers}\times 3}$, true object position $x_{obj}\in\mathbb{R}^3$, true object quaternion $q_{obj}\in\mathbb{R}^4$, true object velocity $v_{obj}\in\mathbb{R}^3$, and true angular velocity $w_{obj}\in\mathbb{R}^3$. 
We define the teacher policy action $a$ as inputs to the underlying geometric fabric, where $a=[x_{f, target}, r_{f, target}, x_{pca, target}]\in\mathbb{R}^{11}$, where $x_{f, target}\in\mathbb{R}^3$ is target palm position, $r_{f, target}\in\mathbb{R}^3$ is the target palm orientation in Euler angles, and $x_{pea, target}\in\mathbb{R}^5$ is the target PCA position for the fingers. The fabric is integerated at 60Hz and the simulation steps at 60Hz. The teacher policy runs at 15Hz, so actions are repeated for intermediate timesteps.

teacher is an MLP + LSTM with skip connections
critic uses MLP because it has access to all privileged information, so it does not need to capture temporal dependencies

##### student distillation
The distillation results in a student policy that uses continuous image input at 15Hz to perform reactive dynamic grasping in the real world. During distillation, the student $\pi_{depth}(o_{depth})\to (\hat{a}, \hat{x}_{obj})$ receives an observation $o_{depth}=[o_{robot}, x_{goal}, I]$, where $I\in [0.5, 1.5]^{160\times120}$m is a raw depth image. It produces actions $\hat{a}\in\mathbb{R}^11$ and object position predictions $\hat{x}_{obj}\in\mathbb{R}^3$. The student is trained with a supervision loss $\mathcal{L}=\mathcal{L}_{action}+\beta\mathcal{L}_{pos}$ where $\mathcal{L}_{action}=\|\hat{a}-a\|_2$ and $\mathcal{L}_{pos}=\|\hat{x}_{obj}-x_{obj}\|_2$ where $a$ are actions predicted by the teacher $\pi_{privileged}$ and $x_{obj}$ are the ground-truth object positions from the simulator.

$o_{robot}$ and $x_{goal}$ are encoded using 3-layer MLPs (512, 256, 128) with elu activation.
$I$ is encoded using three conv layers with increasing depth $16, 32, 64$, kernel size 3, stride 1, padding 1, followed by max-pooling operations (kernel size 2, stride 2) with ReLU activations, and followed by a 2-layer MLP (128, 128) also with ReLU activation.
Encodings of all modalities are concatenated and passed through a GRU layer (1 hidden layer, 1024 units) that predicts $\hat{a}$. 

the student model is a GRU

###### rewards
We define our reward as a weighted sum of individual reward terms $r=\sum_i w_ir_i\in\mathbb{R}$, where $r_i\in\mathbb{R}$ and $w_i\in\mathbb{R}$ are the reward and weight associated with the i-th reward term, respectively. Let $e\in\mathbb{R}$ be an error term we want to minimize and $e_{smallest}\in\mathbb{R}$  be the smallest the error term has been in this episode so far. We define a stateful function $minimize(e) = max(e_{smallest} âˆ’ e, 0)$, which only gives a positive reward if the error term drops below the smallest it has been so far, otherwise it gets no reward. This means that when the reward is positive, $e$ has dropped below $e_{smallest}$, so we subsequently update $e_{smallest}$ so the teacher policy does not get additional reward for staying at the same error in subsequent timesteps.
![[Screenshot 2025-07-06 at 1.28.49 PM.png]]

The environment is reset if the object falls below the table or the robot received the success reward, or if the episode time limit is reached.

###### domain randomization
random wrench perturbations: apply random wrenches that move and rotate the object in unpredictable ways. his forces the policy to learn grasps that are robust to exogeneous perturbations.

pose noise: add uncorrelated and correlated noise that are robust to exogeneous perturbations. This gives incentive to learn grasping behavior that opens that hand wider than typically needed when approaching the object to reduce unexpected contact and account for uncertainty in position and geometry.

friction reduction: reduce the default coefficient of friction of the object ot $\mu=0.7$, mitigating grasping behavior that is overly reliant on friction.

domain randomization: employ domain randomization across simulation parameters to learn policies that are robust across a spectrum of dynamics

