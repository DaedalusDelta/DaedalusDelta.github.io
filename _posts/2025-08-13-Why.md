---
layout: post
title:  "Welcome to My Blog"
date:   2025-08-13
video:  assets/videos/dextrah_blog.mp4
tags:   []
---

Hi, traveler—welcome to an oasis in the desert of scattered information and lurking clickbaits—the labyrinth. Here, you can pause for a moment of peace, still your restless mind, and read about deep learning. You’re free to use the contents however you wish (just remember to cite what you quote from here—thanks!), but as a guide for readers, the sections below briefly explain what you should expect from this blog.

Quick 2 liner summary: This blog is for people interested in understanding the frontiers of deep learning research. Blogs are written as summaries of papers, stories that connects multiple papers, and techniques that shed light on programming quirks and design choices used in practice. 

# What you will find in this blog
Despite the skyrocketing popuarlity of deep learning, learning about this mysterious field is still a tricky business. It's not because deep leanring too advanced and obscure for people like you and me to comprehend. In fact, I'd say that many areas in theoritical fields like math, physical, and clasical mechanics, are much more demanding than deep learning. As an undergrad in AI at Carnegie Mellon and a researcher in deep learning, I know first hand that the problem with learning about deep learning is that the material is too disorganized.

You see, my reader, in most other fields, the decades worth of work produced by researchers are distilled into well-written textbooks that explain the knowledge in a systematic manner. This is not the case with deep learning, because the field is evolving way too rapidly—state of the art get refreshed on a monthly basis, papers from two years ago are already likely to be outdated. By the time you write a book about deep learning, the methods and theories in it are already outdated.

If organized text books don't exist, what else? The only answer is: papers and, spinning off from them, blogs. These sources are often very scattered and document incremental progress, which makes it very difficult to navigate for the common reader. However, I'm only saying that the material is disorganized, but there are actually many well-reasoned and systematic lines of progress as you connect the right papers in the right way. For example, the evolution of contrastive self-supervised methods in computer vision from CLIP to GLIP to BLIP to Grounding Models can be very easily understood as long as you connects the dots, but can you fish out the right ones from the sea of papers? This is where my blog comes in.


# How this blog is organized
This blog has three types of articles: summaries, stories, and techniques. 

Summaries are condensed notes for individual papers. When I was first introduced to deep leanring research, I have no idea how to read a deep learning paper—there's too much going on in a paper and I don't know how to extract the relevant information. This was until my mentor told me that a good paper should tell a story and that this story can be explained in less than 5 sentences that an amateur would understand. For instance, *Attention is All You Need* tells the simple story that "There's no point in using reccurence methods like RNNs and LSTMs, the attention mechanism alone is enough for modeling autoregressive language tasks. My summaries include: 1) a story section that explains the crux of the paper in less than 5 sentences, 2) a substories section that lists quick tips and side stories in the paper, 3) a methods section that elaborates on the details of the paper, 4) a results section that measures how good the results are, 5) a dataset section that tells what data is being used, 6) occasionally a coding section that sheds light on how the structure of code base of the paper.

Stories are the essence of this blog. They connect multiple papers together to paint a clear picture of the evolution of a line of research. I'll depict what methods are used, how these methods are improved or replaced, and explain the motiviation behind them. You're encouraged to read the individual summaries included first before reading a story.

Techniques are quick quirks and tricks used in deep learning. After all, this is a very empirical field that has many engineering aspects. These are the things typically not included in papers and are known by practitioners of the field.


# Who should read this blog
You






# Why I write this blog
This section is purely optional. You can skip it and it wouldn't affect your experience of reading the blogs at all. Read on if you're interested in my vision.
