---
layout: post
title:  "Welcome to My Blog"
date:   2025-08-13
video:  assets/videos/dextrah_blog.mp4
tags:   []
---

# Welcome to My Blog

Hi, traveler — welcome to an oasis in the desert of scattered information and lurking clickbait. Here, you can pause for a moment of peace, still your restless mind, and read about deep learning. You’re free to use the contents however you wish (just remember to cite your quotes — thanks!). The sections below explain what you can expect from this blog.

**Quick 3-liner:** For people who want to understand the *frontiers* of deep learning, the posts include: paper summaries, articles that describe lines of research, and practical techniques. The content is focused on computer vision and robotics, but often relevant more broadly.  

---

## What You’ll Find Here

Despite deep learning's skyrocketing popularity, finding clear, organized knowledge about the field remains surprisingly tricky. This isn’t because the ideas are too advanced. In fact,  I'd say that many areas of mathematics or physics are more demanding. But deep learning evolves so rapidly that its knowledge rarely gets distilled into structured resources.

In most fields, decades of research are refined into textbooks that systematically explain the core concepts. Deep learning doesn’t work that way. State-of-the-art methods can change monthly, and papers from just two years ago may already be obsolete. By the time a book is published, much of it is outdated.

That leaves papers and blogs valuable, but scattered and focused on incremental progress. Beneath the chaos, however, there are clear, systematic lines of research… if you know how to connect the dots.  
For example: the path from CLIP → GLIP → BLIP → grounding models is straightforward if you pick the right papers. But finding them in a sea of publications? That’s where this blog comes in.

---

## How This Blog is Organized
This blog has three types of posts: summaries, stories, and techniques. 

**1. Summaries**  
Concise notes on individual papers. Each includes:  
- **Story:** the core idea in <5 sentences  
- **Substories:** side notes and tips  
- **Methods:** technical details  
- **Results:** performance metrics  
- **Dataset:** data used  
- **Coding:** occasional implementation notes  

When I was first introduced to deep learning research, I have  I have no idea how to read a deep learning paper. There's so much going on in a paper that I don't know what is the relevant information. This was until my mentor told me that “a good paper should tell a story and that this story can be explained in less than 5 sentences”. For example: *Attention Is All You Need* distilled to — “You don’t need recurrence (RNNs, LSTMs); the attention mechanism alone can model sequences of text.” This is how I write a summary for a paper — write down the story and structure the rest around it.

**2. Stories**  
Stories are the essence of this blog. Multi-paper narratives that trace how a research area evolves — methods, improvements, replacements, and motivations. Best read after relevant summaries.

**3. Techniques**  
Engineering tricks and quirks used in practice but often omitted from papers.

---

## Who Should Read This Blog

If you’re looking for beginner-level content like what neural networks are, how CNNs or Transformers work, there are excellent online courses for that. This blog is for the *frontier* stage, where keeping up-to-date is essential. Note that unlike basic sciences, deep learning frontier knowledge has tremendous application value and understanding frontier knowledge is crucial for any deep learning practitioner.

If you’re new, start with the basics and work your way up. This field is new and the gap to the frontier is shorter than you think! This is an exciting field full of surpises. I'm still amazed by new acheivements every week.

If you’ve mastered concepts like forward/backpropagation, optimizers, schedulers, loss functions, and architectures like MLPs, CNNs, RNNs, LSTMs, and Transformers, congrats, you're ready to start reading papers and following frontier research! Use this blog to explore research stories and discover lines of work you enjoy.

---

## Why I Write

I study deep learning full-time and am pursuing a PhD in the field, but I also see myself as a liberal-arts-minded researcher. To me, liberal-arts is not just applying some technical field knowledge to a social science. Liberal-arts means that you have an overarching philosophical framework of this world and you are willing to utilize any form of knowledge to acheive your philosophical goals. At my core I'm a philosopher, and I pursue deep learning as the means to acheive my philosophical end. As for this blog, it is my historian side speaking — a reservoir of thoughts and research knowledge for fellow seekers of knowledge. 

